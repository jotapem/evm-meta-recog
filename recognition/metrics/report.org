* Surveillance Face Recognition Challenge (arxiv 1804.09691)
Introduce a new Surveillance Face Recognition Challenge, QMUL-SurvFace benchmark.

The dataset used is mentioned as a contribution (where is it??).

** Challenge
*** Cool Quotes
"[...] the largest and more importantly the only true surveillance FR benchmark to our best knowledge"

"This challenge contains 463,507 face images of 15,573 distinct identities captured in realworld uncooperative surveillance scenes over wide space and time"

"Currently, the largest surveillance FR challenge benchmark is the UnConstrained College Students [UCCS, from 1708.02337] dataset (...). Moreover, critically the UCCS dataset images were captured by a high-resolution camera at a single location."

"A unique feature of this new surveillance face recognition benchmark dataset is the provision of cross-location (cross camera views) identity label annotations, available from their corresponding person re-identification datasets. This cross-view labelling information can be useful for open-world (vs. existing closed-world) face recognition testing." ????????

"In contrast to existing FR challenges typically considering
a closed-set face identification, we particularly evaluated these algorithms for performing a more realistic open-set surveillance face recognition task. That is, the closed-set test assumes the existence of every probe face ID in the gallery so a true-match always exists for each probe (...)"

"We investigate the effectivenss of existing FR models on native low-resolution surveillance imagery data by exploiting simultaneously image super-resolution (...) and the five representative deep learning face recognition models."

** Deep Learning FR Models
DeepID2
CentreFace
VggFace
FaceNet
SphereFace

** Evaluation and Metrics (sec 3.2)
*** Data Partition
People that have 2 or more face images are evenly split into train and test datasets.
People that have only 1 face image go to the train data split.
Only 1 training data split is used on the evaluation (data is wide enough).


*** Face Verification
Face pairs (1:1)

All classifiers should output a similiarity score
Metrics are threshold functions

**** False Accept Rate (FAR(t))
Percentage of unmatched pairs with a similarity score above t

U unmatched pairs similarity scores
FAR(t) = |{s >=t for s in U}| / |U|

**** False Rejection Rate(FRR(t))
Percentage of matched pairs with a similarity score below t

M matched pairs similarity scores
FRR(t) = |{s < t for s in M}| / |M|

**** True Accept Rate (TAR(t))
Complement of FRR(t)

TAR(t) = 1 - FRR(t)

**** TAR@FAR paired measure
NOT SURE BUT I THINK IT IS
supposed to be read as TAR(FAR)

**** Receiver Operating Characteristic (ROC)
TAR-vs-FAR (FAR on a axe, TAR on the other) curve
Domain (FAR(t) values) are generated by varying t in [0,1]

**** AUC (maybe area under curve? lol)
Area under ROC curve
Scalar way of evaluating ROC


*** Face Identification
A probe image must be compared against ALL gallery identities (1:N)

**** Closed Set
The gallery represents imagery involved in an operational database
Assumes that each probe is present in the gallery
Scenario assumed by most FR methods

For any unique person, a identity-specific face template is generated from either one or multiple images enrolled into the gallery (????? like a face descriptor pooling?)
The above makes the ranking list concise and more efficient for post-rank manual validation (auditoria?). For example, a single identity will not take multiple rank positions.

***** Cumulative Matching Characteristic (CMC(r))
CMC(r) curve reports the fractions of searches that yields the true match for a rank r or better (lower).

Nmate(i) number of probes where true match is found at rank i
N total number of probes
CMC(r) = SUM(i from 1 to r) of Nmate(i) / N

**** Open Set
"In realistic surveillance applications, however, most faces captured by CCTV cameras are not of any gallery person and therefore should be detected as unknown, leading to the open-set face recognition protocol."
"This is often referred to the watch-list identification (forensic search) scenario where only persons of interest are enrolled into the gallery, typically each identity with several different images"

SHOULD READ MORE


* Unconstrained Face Detection and Open-Set Face Recognition Challenge (arxiv 1708.02337)
Evaluation is chapter 4

** Metrics
*** Face Detection

**** Bouding Box Accept [J(G,D)]
Do not use Intersection Over Union exactly, given different methods will have different detectors. The ground truth bounding boxes are relatively large so the "union part" tries to not penalize small detections.
COULD PUT THE EQUATION

(Probably just useful when training multiple detector with different detections size ; still could do something more accurate if these sizes are known)

**** THERE IS MORE!!
    
*** Face Recognition
S- negative group, contains false positives (detected uknowns faces labeled as knowns or mismatches) similarity scores
S+ positive group, contains true positives (detected known faces where predicted label matches ground truth) similiarity scores
N total number of known faces

Note that 'unknown' predictions and undetected faces are treated like the same. 
Also, [ |S-|+|S+| != N  ] since known faces might not be detected or predicted as unknown, the same way unkown faces might be detected.

**** Decision Threshold [T(FI)]
Takes a integer FI that represents how many members of S- would not be hidden by T.

Gives a threshold for similarity scores controlled by how many false identifications are allowed to happen.

T(FI) = argmax(t) |{s >= t for s in S-}| < FI

**** Detected and Identified Rate [DIR]
Given the decision threshold T(FI), counts how many members of S+ would pass the threshold T.
As a rate, the count is divided by N.

DIR(T(FI)) = |{s > T(FI) for s in S+}| / N

DIR is as a function of FI as well.


**** THERE IS MORE
