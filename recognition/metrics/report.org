* [1] Surveillance Face Recognition Challenge (arxiv 1804.09691)
Introduce a new Surveillance Face Recognition Challenge, QMUL-SurvFace benchmark.

The dataset used is mentioned as a contribution (where is it??).

** Challenge
*** Cool Quotes
"[...] the largest and more importantly the only true surveillance FR benchmark to our best knowledge"

"This challenge contains 463,507 face images of 15,573 distinct identities captured in realworld uncooperative surveillance scenes over wide space and time"

"Currently, the largest surveillance FR challenge benchmark is the UnConstrained College Students [UCCS, from 1708.02337] dataset (...). Moreover, critically the UCCS dataset images were captured by a high-resolution camera at a single location."

"A unique feature of this new surveillance face recognition benchmark dataset is the provision of cross-location (cross camera views) identity label annotations, available from their corresponding person re-identification datasets. This cross-view labelling information can be useful for open-world (vs. existing closed-world) face recognition testing." ????????

"In contrast to existing FR challenges typically considering
a closed-set face identification, we particularly evaluated these algorithms for performing a more realistic open-set surveillance face recognition task. That is, the closed-set test assumes the existence of every probe face ID in the gallery so a true-match always exists for each probe (...)"

"We investigate the effectivenss of existing FR models on native low-resolution surveillance imagery data by exploiting simultaneously image super-resolution (...) and the five representative deep learning face recognition models."

** Deep Learning FR Models
DeepID2
CentreFace
VggFace
FaceNet
SphereFace


** Evaluation and Metrics (sec 3.2)
*** Data Partition
People that have 2 or more face images are evenly split into train and test datasets.
People that have only 1 face image go to the train data split.

Only 1 training data split is used on the evaluation (data is wide enough).


*** Face Verification
Face pairs (1:1)

All classifiers should output a similiarity score
Metrics are threshold functions

**** False Accept Rate (FAR(t))
Percentage of unmatched pairs with a similarity score above 't'

U unmatched pairs similarity scores
FAR(t) = |{s >=t for s in U}| / |U|

**** False Rejection Rate(FRR(t))
Percentage of matched pairs with a similarity score below 't'

M matched pairs similarity scores
FRR(t) = |{s < t for s in M}| / |M|

**** True Accept Rate (TAR(t))
Complement of FRR(t)

TAR(t) = 1 - FRR(t)

**** TAR@FAR paired measure
NOT SURE BUT I THINK IT IS
supposed to be read as TAR(FAR)
A TAR value when a FAR value is fixed

**** Receiver Operating Characteristic (ROC)
TAR-vs-FAR (FAR on a axe, TAR on the other) curve
Domain (FAR(t) values) are generated by varying 't' in [0,1]

**** AUC (maybe area under curve? lol)
Area under ROC curve
Scalar way of evaluating ROC


*** Face Identification
A probe image must be compared against ALL gallery identities (1:N)

**** Closed Set
The gallery represents imagery involved in an operational database
Assumes that each probe is present in the gallery
Scenario assumed by most FR methods

For any unique person, a identity-specific face template is generated from either one or multiple images enrolled into the gallery (????? like a face descriptor pooling?)
The above makes the ranking list concise and more efficient for post-rank manual validation (auditoria?). For example, a single identity will not take multiple rank positions.

***** Cumulative Matching Characteristic (CMC(r))
CMC(r) curve reports the fractions of searches that yields the true match for a rank 'r' or better (lower).

Nmate(i) number of probes where true match is found at rank 'i'
N total number of probes
CMC(r) = SUM(i from 1 to r) of Nmate(i) / N

**** Open Set
"In realistic surveillance applications, however, most faces captured by CCTV cameras are not of any gallery person and therefore should be detected as unknown, leading to the open-set face recognition protocol."
"This is often referred to the watch-list identification (forensic search) scenario where only persons of interest are enrolled into the gallery, typically each identity with several different images"
"For the open-set FR performance evaluation, we must quantify two error types (Grother and Ngan, 2014)."

***** False Positive Identification Rate (FPIR(t))
False alarm: face image from a unknown associated to one or more enrollee's data.

Nnm,m unknown probes (nonmate searches) that produce one or more enrolled candidates at or above 't'
Nnm total nonmate searches
FPIR(t) = Nnm,m / Nnm

***** False Negative Identification Rate (FNIR(t,r))
Miss: a search for a enrolled target that does not return its correct identity

Nm,nm probes of targets (mate searches) where the correct identity similarity score is below 't' or is not captured by rank 'r'
Nm total mate searches
FNIR(t,r) = Nm,nm / Nm

"By default, we set r = 20 (i.e. FNIR(t, 20)) which assumes a small workload by a human reviewer employed to review the candidates returned from an identification search"

***** True Positive Identification Rate (TPIR(t,r))
Complement of FNIR(t,r)

TPIR(t,r) = 1 - FNIR(t,r)

***** TPIR@FPIR Paired Measure
Similar to TAR@FAR

***** ROC and AUC
Similar to Face Verification TAR-vs-FAR manipulations

**** Link between open-set and closed-set
CMC(r) can be regarded as a special case of TPIR(t,r), ignoring similarity scores by relaxing 't'.


* [2] Unconstrained Face Detection and Open-Set Face Recognition Challenge (arxiv 1708.02337)
Evaluation is chapter 4

** Metrics
*** Face Detection
C+ positive group, contains accepted similarity scores for ground-truth boxes
C- negative group, contains rejected similarity scores for ground-truth boxes
M total number of labeled faces (ground-truth values)

Note that |C+|+|C-| = M
Also, C- does not capture 'extra' false detections (e.g 10 detections to 5 ground-truth values) as long as the ground-truth values are correctly matches

**** Bouding Box Accept [J(G,D)]
Do not use Intersection Over Union exactly, given different methods will have different detectors. The ground truth bounding boxes are relatively large so the "union part" tries to not penalize small detections.
COULD PUT THE EQUATION

(Probably just useful when training multiple detector with different detections size ; still could do something more accurate if these sizes are known)

**** Detection Threshold [T(FA)]
Given a (absolute) False Accept number, compute a similarity score threshold

T = argmax t where |{c >= t for c in C-}| < FA

**** Detection Rate [DR]
Uses detection threshold T(FA) to filter C+.

DR(T(FA)) = |{c >= T(FA) for c in C+}| / M

    
*** Face Recognition
S- negative group, contains false positives (detected uknowns faces labeled as knowns or mismatches) similarity scores
S+ positive group, contains true positives (detected known faces where predicted label matches ground truth) similiarity scores
N total number of known faces

Note that 'unknown' predictions and undetected faces are treated like the same. 
Also, |S-|+|S+| != N, since known faces might not be detected or predicted as unknown, the same way unkown faces might be detected.

**** Decision Threshold [T(FI)]
Takes a integer FI that represents how many members of S- would not be hidden by T.

Gives a threshold for similarity scores controlled by how many false identifications are allowed to happen.

T(FI) = argmax(t) |{s >= t for s in S-}| < FI

**** Detected and Identified Rate [DIR]
Given the decision threshold T(FI), counts how many members of S+ would pass the threshold T.
As a rate, the count is divided by N.

DIR(T(FI)) = |{s >= T(FI) for s in S+}| / N

Note that DIR is as a function of FI as well.


*** Analysis of False Identifications
"In the evaluations above, false identifications are computed jointly from unknown faces and false accepts (misdetections)."

S= negative group, contains similarity scores only from detected unknown faces recognitions (a subset of S-)

**** Correct Rejection Rate [CRR]
Decision threshold T(FI) is now computed at S+ instead of S-.

Captures how much the classifier fails to reject unknowns as the identification rate requirement gets higher.

CRR(T(FI)) = |{s < T(FI) for s in S=}| / |S=|
     
* [3] Low-shot Face Recognition with Hybrid Classifiers

** Metrics

'j' correctly recognized images (matches)
'm' recognized images (positive predictions)
'k' total number of images

*** Precision
precision = j/m
*** Coverage
coverage = m/k

*** Coverage at Precision (C@P)
Similar to Tx@Fx paired measures of [1]. 
A Precision rate is fixed, then Coverage is evaluated.

INTUITION
Precision measures how confident the classifier predictions are, how much it gets its predictions right when it tries to predict (does not capture the evasive behaviour of predicting everything as unknown)
Coverage measures how confident the classifier is with its own predictions, how much of the available samples it actually tries to make a true prediction (does not capture how good these predictions are)

Therefore, C@P measures the rate of samples the classifier will attempt to predict given a fixed 'prediction quality'.



* [4] One-shot Face Recognition by Promoting Underrepresented Classes (arxiv 1707.05574)
** Cool Quotes
"We test our solution on the MS-Celeb-1M low-shot learning benchmark task. Our solution recognizes 94.89% of the test images at the precision of 99% for the one-shot classes."

** Coverage at Precision (C@P)
Similar to [3].

